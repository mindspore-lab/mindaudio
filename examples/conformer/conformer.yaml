# network architecture
# encoder related
encoder: conformer
encoder_conf:
    output_size: 256    # dimension of attention
    attention_heads: 4
    linear_units: 2048  # the number of units of position-wise feed forward
    num_blocks: 12      # the number of encoder blocks
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    attention_dropout_rate: 0
    input_layer: conv2d # encoder input type, you can chose conv2d, conv2d6 and conv2d8
    normalize_before: True
    cnn_module_kernel: 15
    activation_type: 'swish'
    pos_enc_layer_type: 'rel_pos'
    feature_norm : True

# decoder related
decoder: transformer
decoder_conf:
    attention_heads: 4
    linear_units: 2048
    num_blocks: 6
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    self_attention_dropout_rate: 0
    src_attention_dropout_rate: 00

# hybrid CTC/attention
model_conf:
    ctc_weight: 0.3
    lsm_weight: 0.1     # label smoothing option
    length_normalized_loss: False

# feature extraction
collate_conf:
    # feature level config
    feature_extraction_conf:
        feature_type: 'fbank'
        mel_bins: 80
        frame_shift: 10
        frame_length: 25
        using_pitch: False
    feature_dither: 0.0 # add dither [-feature_dither,feature_dither] on fbank feature
    # data augmentation config
    use_speed_perturb: True
    use_spec_aug: True
    spec_aug_conf:
        warp_for_time: False
        num_t_mask: 2
        num_f_mask: 2
        prop_mask_t: 0.1
        prop_mask_f: 0.1
        max_t: 50
        max_f: 10
        max_w: 80
    use_dynamic_chunk: False
    use_dynamic_left_chunk: False
    decoding_chunk_size: 0
    static_chunk_size: 0
    num_decoding_left_chunks: -1

# dataset related
dataset_conf:
    max_length: 3000
    min_length: 0
    token_max_length: 30
    token_min_length: 1
    batch_type: 'bucket'    # bucket, static, dynamic
    frame_bucket_limit: '144, 204, 288, 400, 512, 600, 712, 800, 912, 1024, 1112, 1200, 1400, 1600, 2000, 3000'
    batch_bucket_limit: '40, 80, 80, 72, 72, 56, 56, 56, 40, 40, 40, 40, 24, 8, 8, 8'
    batch_factor: 1
    shuffle: True

# train option
grad_clip: 5
accum_grad: 1
max_epoch: 240
log_interval: 100

optim: adam
optim_conf:
    lr: 0.001
scheduler: warmuplr
scheduler_conf:
    warmup_steps: 25000

cmvn_file: "/disk1/huangyu/data/data_aishell/global_cmvn"
is_json_cmvn: True

exp_name: default
train_data: "/disk1/huangyu/data/train.csv"
eval_data: "/disk1/huangyu/data/dev.csv"
save_checkpoint: True
save_checkpoint_epochs: 1
save_checkpoint_steps: 460
keep_checkpoint_max: 30
save_checkpoint_path: "./"
device_target: "Ascend"
is_distributed: True
mixed_precision: True
resume_ckpt: ""
save_graphs: False
training_with_eval: True

# decode option
test_data: "/disk1/huangyu/data/test.csv"
dict: "/disk1/huangyu/data/test.txt"
decode_ckpt: "/disk1/huangyu/code/06coding_online/mindaudio/examples/conformer/output0/default/model/conformer_avg_30.ckpt"
decode_mode: "ctc_greedy_search" # ctc_greedy_search,ctc_prefix_beam_search,attention,attention_rescoring
full_graph: True
decode_batch_size: 1
ctc_weight: 0.0
beam_size: 10
penalty: 0.0

test_dataset_conf:
    max_length: 1200
    min_length: 0
    token_max_length: 30
    token_min_length: 1
    batch_type: 'bucket'    # bucket, static, dynamic
    frame_bucket_limit: '1200'
    batch_bucket_limit: '40'
    batch_factor: 1
    shuffle: False

